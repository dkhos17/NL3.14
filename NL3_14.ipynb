{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL3.14.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FhIIZyj4v4-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83621c5-2235-4587-e5d6-80c77dd1586e"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-JKIMU00I-r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45f5143c-c899-4c86-8179-1d16a66220af"
      },
      "source": [
        "import os\r\n",
        "import re\r\n",
        "import torch\r\n",
        "import glob\r\n",
        "import string\r\n",
        "import math\r\n",
        "import random\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import zipfile\r\n",
        "import seaborn as sbr\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import pandas as pd\r\n",
        "import functools\r\n",
        "from tqdm import tqdm\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "SEED = 147\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgMLAOolGS8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c969fc88-1870-4565-8fc9-f2f7547949b4"
      },
      "source": [
        "!pip install --upgrade pip\r\n",
        "!pip install transformers\r\n",
        "!pip install bpemb\r\n",
        "\r\n",
        "from bpemb import BPEmb\r\n",
        "from transformers import BertTokenizer, BertModel, AutoTokenizer, AdamW, BertForMaskedLM, Adafactor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (20.3.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.6/dist-packages (0.3.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from bpemb) (0.1.95)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (4.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1qjh3xHIMmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44aa19b7-760d-41ea-e415-2111aa536ffb"
      },
      "source": [
        "torch.cuda.empty_cache()\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjQbGSs3rO2X"
      },
      "source": [
        "total_df = pd.read_csv(\"/content/gdrive/My Drive/NL3.14/resources/geosentences.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbNWkU2g1ree"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "from gensim.test.utils import get_tmpfile\r\n",
        "\r\n",
        "fname = get_tmpfile(\"/content/gdrive/My Drive/NL3.14/resources/word2vec.model\")\r\n",
        "\r\n",
        "w2v = Word2Vec.load(fname)\r\n",
        "# w2v = Word2Vec(sentences=train_data, sg=1, hs=1, size=300, workers=4, window=3, seed=SEED, sorted_vocab=1, min_count=3)\r\n",
        "\r\n",
        "# w2v.save(fname)\r\n",
        "# w2v.train(train_data, total_examples=len(train_data), epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfVHAvhh0MyI"
      },
      "source": [
        "final_df = pd.read_csv(\"/content/gdrive/My Drive/NL3.14/resources/train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y42_k0Rf0Oe6"
      },
      "source": [
        "bert_train, bert_validation = train_test_split(total_df, test_size=0.05)\r\n",
        "train, validation = train_test_split(final_df, test_size=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dB5dqi_U5AUJ",
        "outputId": "e51debb0-e848-46a5-8f73-4a003739e24f"
      },
      "source": [
        "train.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1040202</th>\n",
              "      <td>['და', 'კმარა', ',', 'თავი', 'დამანებე', ',']</td>\n",
              "      <td>მასარაკშ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127562</th>\n",
              "      <td>['ამიტომ', 'დანიელი', ',', 'ემიგრანტეივით']</td>\n",
              "      <td>სატვირთო</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390489</th>\n",
              "      <td>['ეზოს', 'გავხედე', ',', 'ის', 'დიდი', 'იყო', ...</td>\n",
              "      <td>ერთ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597724</th>\n",
              "      <td>['სამაგიეროდ', 'პატარა', 'საბაბიც', 'კი', 'საკ...</td>\n",
              "      <td>თავის</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736530</th>\n",
              "      <td>['ხოლო', 'ჩვენს', 'მიერ', 'დაფუძნებულ', 'სახელ...</td>\n",
              "      <td>მცველები</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987744</th>\n",
              "      <td>['იმ', 'თავითვე', ',', 'დავბადებულვარ', 'თუ', ...</td>\n",
              "      <td>გაგდებული</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>934493</th>\n",
              "      <td>['ჟერვეზა', 'ამ', 'თავაზიანობის', 'საპასუხოდ',...</td>\n",
              "      <td>სახურავის</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>590549</th>\n",
              "      <td>['ჩვენ', 'უბრალოდ', ',', 'თვალმოუშორებლივ', 'ვ...</td>\n",
              "      <td>ცეცხლის</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338362</th>\n",
              "      <td>['როგორც', 'ყოველთვის', ',', 'ჯიბეები', 'გამოტ...</td>\n",
              "      <td>მისთვის</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1022993</th>\n",
              "      <td>['შინ', 'მოსულს', 'ფეხსაცმელს', 'გავხდი', 'და']</td>\n",
              "      <td>დაჩოქილი</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797327</th>\n",
              "      <td>['რასაკვირველია', ',']</td>\n",
              "      <td>სრულიად</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>850181</th>\n",
              "      <td>['მოხუცებულმა', 'ხელი', 'გაუწოდა', 'და']</td>\n",
              "      <td>მკლავი</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>962937</th>\n",
              "      <td>['ჟიულის', 'ყველაფერი', 'პირიქით', 'მოუხდა', '...</td>\n",
              "      <td>მისთვის</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1040671</th>\n",
              "      <td>['ღვთის', 'ნაბოძებად', 'თვლის', 'იმ']</td>\n",
              "      <td>მოსავალს</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>710202</th>\n",
              "      <td>['აი', 'მაშინ', 'კი', 'აუცილებლად', 'შემიყვარდ...</td>\n",
              "      <td>ახლაც</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255363</th>\n",
              "      <td>['დაიხრიალა', 'ბოლოს', 'ზეფმა', ',', 'აი', ','...</td>\n",
              "      <td>რა</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210500</th>\n",
              "      <td>['ამრიგად', ',', 'მე', 'ვეთანხმები', 'ხმების',...</td>\n",
              "      <td>რომ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521412</th>\n",
              "      <td>['დედანი', 'რუსომ', 'მოსპო', ',']</td>\n",
              "      <td>რადგან</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>856672</th>\n",
              "      <td>['აი', 'შენი', 'ცნობები', ',', 'შე']</td>\n",
              "      <td>არამზადავ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652484</th>\n",
              "      <td>['დანტესმა', 'ყურები', 'ცქვიტა', ':', 'მას', '...</td>\n",
              "      <td>შემოესმა</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48702</th>\n",
              "      <td>['წინ', ',', 'დივანზე', 'კი', 'სასამართლოს', '...</td>\n",
              "      <td>ნიკოლაი</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201728</th>\n",
              "      <td>['გმადლობთ', ',', 'მისტერ']</td>\n",
              "      <td>სენდერსონ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999536</th>\n",
              "      <td>['მანამკ', 'იქამდე', 'მივეთრიეთ', ',', 'მჭედელ...</td>\n",
              "      <td>სამმა</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>977855</th>\n",
              "      <td>['ბრძოლის', 'ბედი', ',', 'რამაც', 'დაუდო', 'სა...</td>\n",
              "      <td>გამარჯვებას</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984016</th>\n",
              "      <td>['სახელოვანი', 'ბერძენი', 'გმირი', ',']</td>\n",
              "      <td>რომელსაც</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601915</th>\n",
              "      <td>['უკანასკნელ', 'ხანებში', 'ყოველღამე', 'იქ', '...</td>\n",
              "      <td>კი</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6490</th>\n",
              "      <td>['თუ', ',', 'შენის', 'აზრით', ',']</td>\n",
              "      <td>სულაც</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169586</th>\n",
              "      <td>['ვალენტინა', 'ბაბუას', 'მადლიერების', 'ღიმილით']</td>\n",
              "      <td>შეჰყურებდა</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71936</th>\n",
              "      <td>['დაპირისპირებას', 'იმ', 'ადამიანთა', 'ჯგუფს',...</td>\n",
              "      <td>იმათაც</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632106</th>\n",
              "      <td>['განცვიფრებული', 'და', 'გაოგნებული', 'შიკი', ...</td>\n",
              "      <td>მიეფარა</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>674836</th>\n",
              "      <td>['მოდა', 'მალფუჭებადი', 'პროდუქტია', ':', 'მოდ...</td>\n",
              "      <td>სამი</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750809</th>\n",
              "      <td>['ნეტავი', 'რად', 'არაა', 'სიხარული', 'ესოდენ'...</td>\n",
              "      <td>როგორიც</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>731307</th>\n",
              "      <td>['ექვსი', 'ოფიცერი', ',', 'გაცრეცილი', ',', 'ო...</td>\n",
              "      <td>წყალზე</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>692643</th>\n",
              "      <td>['და', 'განაგრძო', ':', 'აი', ',', 'ჩემო', 'მწ...</td>\n",
              "      <td>მინდა</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57472</th>\n",
              "      <td>['ასე', 'თქვა', ',', 'ორ']</td>\n",
              "      <td>საათში</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197177</th>\n",
              "      <td>['ვინ', 'იცის', ',', 'კვირამდე']</td>\n",
              "      <td>რა</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45481</th>\n",
              "      <td>['განადგურება', 'მეჩვენებოდა', ',', 'რომ', 'ბო...</td>\n",
              "      <td>ისევ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138722</th>\n",
              "      <td>['კარგი', ',']</td>\n",
              "      <td>მაშინ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>768422</th>\n",
              "      <td>['სხვას', 'ყველას', ',']</td>\n",
              "      <td>გარდა</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888696</th>\n",
              "      <td>[',', 'მაგრამ', 'გარდა']</td>\n",
              "      <td>ამისას</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1038228</th>\n",
              "      <td>['იმას', 'საჩქარო', 'შეკვეთა']</td>\n",
              "      <td>ჰქონდა</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412647</th>\n",
              "      <td>['საქმე', 'ის', 'გახლავთ', ',', 'კომისარო', ',']</td>\n",
              "      <td>რომ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892102</th>\n",
              "      <td>['მე', 'ვსპობ', 'მის', 'კავშირს', 'აქაურობასთა...</td>\n",
              "      <td>მიმყავს</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143407</th>\n",
              "      <td>['საქმე', 'ეკისრა', ',', 'თუმცა', 'საქმე', 'არ...</td>\n",
              "      <td>ძალას</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>611279</th>\n",
              "      <td>['თუმცა', ',', 'კაცი', '222', 'სანახავად', 'არც']</td>\n",
              "      <td>ისე</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019056</th>\n",
              "      <td>['მაშინვე', 'მივხვდი', ',', 'რომ', 'ფუსუნი', '...</td>\n",
              "      <td>სირცხვილისაგან</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742529</th>\n",
              "      <td>['მინისტრმა', 'ინება', ',', 'თვითონვე', 'დარწმ...</td>\n",
              "      <td>შემდეგ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187179</th>\n",
              "      <td>['ხათუთამ', 'პირი', 'მომარიდა', 'და', 'იმ']</td>\n",
              "      <td>ღამეს</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747329</th>\n",
              "      <td>['ჰო', ',', 'დედიშობილა', 'ენრიკე', 'რომ', 'და...</td>\n",
              "      <td>რამეს</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1095861</th>\n",
              "      <td>['ჯვრის', 'რვიანზე', ',', 'მაგალითად', ',', 'გ...</td>\n",
              "      <td>ჰქონდა</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         x               y\n",
              "1040202      ['და', 'კმარა', ',', 'თავი', 'დამანებე', ',']        მასარაკშ\n",
              "127562         ['ამიტომ', 'დანიელი', ',', 'ემიგრანტეივით']        სატვირთო\n",
              "390489   ['ეზოს', 'გავხედე', ',', 'ის', 'დიდი', 'იყო', ...             ერთ\n",
              "597724   ['სამაგიეროდ', 'პატარა', 'საბაბიც', 'კი', 'საკ...           თავის\n",
              "736530   ['ხოლო', 'ჩვენს', 'მიერ', 'დაფუძნებულ', 'სახელ...        მცველები\n",
              "987744   ['იმ', 'თავითვე', ',', 'დავბადებულვარ', 'თუ', ...       გაგდებული\n",
              "934493   ['ჟერვეზა', 'ამ', 'თავაზიანობის', 'საპასუხოდ',...       სახურავის\n",
              "590549   ['ჩვენ', 'უბრალოდ', ',', 'თვალმოუშორებლივ', 'ვ...         ცეცხლის\n",
              "338362   ['როგორც', 'ყოველთვის', ',', 'ჯიბეები', 'გამოტ...         მისთვის\n",
              "1022993    ['შინ', 'მოსულს', 'ფეხსაცმელს', 'გავხდი', 'და']        დაჩოქილი\n",
              "797327                              ['რასაკვირველია', ',']         სრულიად\n",
              "850181            ['მოხუცებულმა', 'ხელი', 'გაუწოდა', 'და']          მკლავი\n",
              "962937   ['ჟიულის', 'ყველაფერი', 'პირიქით', 'მოუხდა', '...         მისთვის\n",
              "1040671              ['ღვთის', 'ნაბოძებად', 'თვლის', 'იმ']        მოსავალს\n",
              "710202   ['აი', 'მაშინ', 'კი', 'აუცილებლად', 'შემიყვარდ...           ახლაც\n",
              "255363   ['დაიხრიალა', 'ბოლოს', 'ზეფმა', ',', 'აი', ','...              რა\n",
              "210500   ['ამრიგად', ',', 'მე', 'ვეთანხმები', 'ხმების',...             რომ\n",
              "521412                   ['დედანი', 'რუსომ', 'მოსპო', ',']          რადგან\n",
              "856672                ['აი', 'შენი', 'ცნობები', ',', 'შე']       არამზადავ\n",
              "652484   ['დანტესმა', 'ყურები', 'ცქვიტა', ':', 'მას', '...        შემოესმა\n",
              "48702    ['წინ', ',', 'დივანზე', 'კი', 'სასამართლოს', '...         ნიკოლაი\n",
              "201728                         ['გმადლობთ', ',', 'მისტერ']       სენდერსონ\n",
              "999536   ['მანამკ', 'იქამდე', 'მივეთრიეთ', ',', 'მჭედელ...           სამმა\n",
              "977855   ['ბრძოლის', 'ბედი', ',', 'რამაც', 'დაუდო', 'სა...     გამარჯვებას\n",
              "984016             ['სახელოვანი', 'ბერძენი', 'გმირი', ',']        რომელსაც\n",
              "601915   ['უკანასკნელ', 'ხანებში', 'ყოველღამე', 'იქ', '...              კი\n",
              "6490                    ['თუ', ',', 'შენის', 'აზრით', ',']           სულაც\n",
              "169586   ['ვალენტინა', 'ბაბუას', 'მადლიერების', 'ღიმილით']      შეჰყურებდა\n",
              "71936    ['დაპირისპირებას', 'იმ', 'ადამიანთა', 'ჯგუფს',...          იმათაც\n",
              "632106   ['განცვიფრებული', 'და', 'გაოგნებული', 'შიკი', ...         მიეფარა\n",
              "674836   ['მოდა', 'მალფუჭებადი', 'პროდუქტია', ':', 'მოდ...            სამი\n",
              "750809   ['ნეტავი', 'რად', 'არაა', 'სიხარული', 'ესოდენ'...         როგორიც\n",
              "731307   ['ექვსი', 'ოფიცერი', ',', 'გაცრეცილი', ',', 'ო...          წყალზე\n",
              "692643   ['და', 'განაგრძო', ':', 'აი', ',', 'ჩემო', 'მწ...           მინდა\n",
              "57472                           ['ასე', 'თქვა', ',', 'ორ']          საათში\n",
              "197177                    ['ვინ', 'იცის', ',', 'კვირამდე']              რა\n",
              "45481    ['განადგურება', 'მეჩვენებოდა', ',', 'რომ', 'ბო...            ისევ\n",
              "138722                                      ['კარგი', ',']           მაშინ\n",
              "768422                            ['სხვას', 'ყველას', ',']           გარდა\n",
              "888696                            [',', 'მაგრამ', 'გარდა']          ამისას\n",
              "1038228                     ['იმას', 'საჩქარო', 'შეკვეთა']          ჰქონდა\n",
              "412647    ['საქმე', 'ის', 'გახლავთ', ',', 'კომისარო', ',']             რომ\n",
              "892102   ['მე', 'ვსპობ', 'მის', 'კავშირს', 'აქაურობასთა...         მიმყავს\n",
              "143407   ['საქმე', 'ეკისრა', ',', 'თუმცა', 'საქმე', 'არ...           ძალას\n",
              "611279   ['თუმცა', ',', 'კაცი', '222', 'სანახავად', 'არც']             ისე\n",
              "1019056  ['მაშინვე', 'მივხვდი', ',', 'რომ', 'ფუსუნი', '...  სირცხვილისაგან\n",
              "742529   ['მინისტრმა', 'ინება', ',', 'თვითონვე', 'დარწმ...          შემდეგ\n",
              "187179         ['ხათუთამ', 'პირი', 'მომარიდა', 'და', 'იმ']           ღამეს\n",
              "747329   ['ჰო', ',', 'დედიშობილა', 'ენრიკე', 'რომ', 'და...           რამეს\n",
              "1095861  ['ჯვრის', 'რვიანზე', ',', 'მაგალითად', ',', 'გ...          ჰქონდა"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP6sXoa4FnYz"
      },
      "source": [
        "embed_dim = 300\r\n",
        "vocab_size = w2v.wv.vectors.shape[0]\r\n",
        "max_seq_len = 32\r\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-jNPjrTUTM1"
      },
      "source": [
        "class DatasetTrain(torch.utils.data.Dataset):\r\n",
        "  def __init__(self, txts):\r\n",
        "        self.txts = txts\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "        return len(self.txts)\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "        sentence = self.txts[index]\r\n",
        "        splits = sentence.split(' ')\r\n",
        "        chosenWords = random.choices(splits, k = len(splits) // 10 + 1)\r\n",
        "        for idx, word in enumerate(splits):\r\n",
        "          if word in chosenWords:\r\n",
        "            splits[idx] = '[MASK]'\r\n",
        "  \r\n",
        "        embedX = tokenizer(' '.join(splits),  padding='max_length', return_tensors='pt', max_length=max_seq_len, truncation=True)\r\n",
        "        embedY = tokenizer(sentence, padding='max_length', return_tensors='pt', max_length=max_seq_len, truncation=True)\r\n",
        "      \r\n",
        "        return embedX['input_ids'].to(device),embedX['token_type_ids'].to(device), embedX['attention_mask'].to(device), embedY['input_ids'].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvvcKF1iVDmi"
      },
      "source": [
        "# Parameters\r\n",
        "train_params = {'batch_size': batch_size,\r\n",
        "          'shuffle': True\r\n",
        "         }\r\n",
        "val_params = {'batch_size': batch_size\r\n",
        "         }\r\n",
        "\r\n",
        "# Dataloaders for train, validation and test\r\n",
        "bert_training_set = DatasetTrain(bert_train['sentences'].tolist())\r\n",
        "bert_training_generator = torch.utils.data.DataLoader(bert_training_set, **train_params)\r\n",
        "\r\n",
        "bert_validation_set = DatasetTrain(bert_validation['sentences'].tolist())\r\n",
        "bert_validation_generator = torch.utils.data.DataLoader(bert_validation_set, **val_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aselDcQ_bsVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4234d9-581f-46a8-8533-faf2318de453"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\r\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased').to(device)\r\n",
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/NL3.14/resources/bert_model'))\r\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\r\n",
        "\r\n",
        "#Calculate loss on validation data\r\n",
        "def bert_valid_loss(model, dl):\r\n",
        "  #Switch model to evaluation mode and then back to train\r\n",
        "  model.eval()\r\n",
        "  with torch.no_grad(): # tells Pytorch not to store values of intermediate computations for backward pass because we not gonna need gradients.\r\n",
        "    loss = 0\r\n",
        "    batches = math.ceil(len(dl) / batch_size)\r\n",
        "    for x_input_ids, x_token_type_ids, x_attention_mask, y_input_ids in bert_validation_generator:\r\n",
        "        output = model(input_ids=x_input_ids.squeeze(1),\r\n",
        "                     token_type_ids=x_token_type_ids.squeeze(1),\r\n",
        "                     attention_mask=x_attention_mask.squeeze(1),\r\n",
        "                     labels=y_input_ids,\r\n",
        "                    )\r\n",
        "        loss += output.loss.item()\r\n",
        "  model.train()\r\n",
        "  return loss / batches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8M_C2Ieb_5z"
      },
      "source": [
        "# from datetime import datetime\r\n",
        "# # Switch model to train mode\r\n",
        "# model.train()\r\n",
        "\r\n",
        "# validation_loss = None\r\n",
        "# for epoch in range(1, 2):\r\n",
        "#     epoch_loss = 0\r\n",
        "#     epoch_acc = 0\r\n",
        "#     epoch_f_score = 0\r\n",
        "#     batches = math.ceil(len(train) / batch_size)\r\n",
        "#     print(batches, \"batches\")\r\n",
        "#     t = datetime.now()\r\n",
        "#     i = 0\r\n",
        "#     for x_input_ids, x_token_type_ids, x_attention_mask, y_input_ids in bert_training_generator:\r\n",
        "        \r\n",
        "#         # if we don't do this, pytorch will accumulate gradients (summation) from previous backprop to next backprop and so on...\r\n",
        "#         # this behaviour is useful when we can't fit many samples in memory but we need to compute gradient on large batch. In this case, we simply accumulate gradient and do backprop\r\n",
        "#         # after enough samples has been seen.\r\n",
        "#         optimizer.zero_grad()\r\n",
        "#         #forward\r\n",
        "#         # Model calculates loss and also outputs classification scores, which need to go through softmax later\r\n",
        "#         output = model(input_ids=x_input_ids.squeeze(1),\r\n",
        "#                      token_type_ids=x_token_type_ids.squeeze(1),\r\n",
        "#                      attention_mask=x_attention_mask.squeeze(1),\r\n",
        "#                      labels=y_input_ids,\r\n",
        "#                     )\r\n",
        "#         # print(output.loss.item())\r\n",
        "#         #calculate accuracy\r\n",
        "#         # acc = calculate_accuracy(log, y)\r\n",
        "\r\n",
        "#         #Back propagation\r\n",
        "#         output.loss.backward()\r\n",
        "        \r\n",
        "#         #Gradient step\r\n",
        "#         optimizer.step()\r\n",
        "\r\n",
        "#         epoch_loss += output.loss.item()\r\n",
        "#         # epoch_acc += acc.item()\r\n",
        "        \r\n",
        "#         if (i + 1) % 1000 == 0:\r\n",
        "#             print(datetime.now() - t)\r\n",
        "#             dev_loss = bert_valid_loss(model, bert_validation)\r\n",
        "#             if validation_loss is None or validation_loss > dev_loss:\r\n",
        "#                 validation_loss = dev_loss\r\n",
        "#                 # Save best model\r\n",
        "#                 torch.save(model.state_dict(), '/content/gdrive/My Drive/NL3.14/resources/bert_model') \r\n",
        "#             print(f'Epoch {epoch} batch {i} | Avg Train Loss: {epoch_loss/(i + 1):.6f} | Current Dev Loss {dev_loss:.6f} | Minimal Dev Loss {validation_loss:.6f}')\r\n",
        "#         i+=1\r\n",
        "#     print(f'Epoch {epoch} | Avg Train Loss: {epoch_loss/batches:.6f} ')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IOfdqKlycIH"
      },
      "source": [
        "batch_size = 256\r\n",
        "\r\n",
        "class PredictionDatasetTrain(torch.utils.data.Dataset):\r\n",
        "  def __init__(self, x, y):\r\n",
        "        self.x = x\r\n",
        "        self.y = y\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "        return len(self.y)\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "        def toEmbed(word):\r\n",
        "          try:\r\n",
        "            return w2v.wv.vocab[word].index\r\n",
        "          except:\r\n",
        "            try:\r\n",
        "              return w2v.wv.vocab[w2v.wv.most_similar(word)[0][0]].index\r\n",
        "            except:\r\n",
        "              return random.randint(0, len(w2v.wv.vectors) - 1)\r\n",
        "        x = [toEmbed(i) for i in self.x[index]]\r\n",
        "        x = (x + [0]*max_seq_len)[:max_seq_len]\r\n",
        "        # y = torch.LongTensor([w2v.wv.vocab[self.y[index]].index])\r\n",
        "        y = nn.functional.one_hot(torch.LongTensor([w2v.wv.vocab[self.y[index]].index]), len(w2v.wv.vocab))\r\n",
        "        return torch.LongTensor(x).to(device), y.to(device).float()\r\n",
        "\r\n",
        "# Dataloaders for train, validation\r\n",
        "next_training_set = PredictionDatasetTrain(train['x'].tolist(), train['y'].tolist())\r\n",
        "next_training_generator = torch.utils.data.DataLoader(next_training_set, **train_params)\r\n",
        "\r\n",
        "next_validation_set = PredictionDatasetTrain(validation['x'].tolist(), validation['y'].tolist())\r\n",
        "next_validation_generator = torch.utils.data.DataLoader(next_validation_set, **train_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHSE2PT9uNRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d7234a-0dba-4958-b3e9-dd500f5e4d62"
      },
      "source": [
        "class PredictionModel(nn.Module):\r\n",
        "    def __init__(self, emb_dim, hid_dim, vocab_size):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.hidden_dim = hid_dim\r\n",
        "        self.emb_dim = embed_dim\r\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(w2v.wv.vectors))\r\n",
        "\r\n",
        "        self.lstm = nn.LSTM(self.emb_dim, self.hidden_dim, dropout=0.2, num_layers=2, bidirectional=True, batch_first=True)\r\n",
        "        \r\n",
        "        # self.lstm2 = nn.LSTM(self.hidden_dim * 4, self.hidden_dim * 2, batch_first=True)\r\n",
        "    \r\n",
        "        #last classifier is linear and we don't need sigmoid since loss function that we are going to use does apply sigmoid\r\n",
        "        self.classifier = nn.Linear(4 * hid_dim, vocab_size)\r\n",
        "              \r\n",
        "    def forward(self, src):\r\n",
        "        #src = [batch size, src len, emb dim]\r\n",
        "        #embedded = [batch size, src len, emb dim]\r\n",
        "        embedded = self.embedding(src)\r\n",
        "        _, (hidden1, _) = self.lstm(embedded)\r\n",
        "        # print(hidden1.shape)\r\n",
        "        hidden1 = hidden1.reshape(-1, 4*self.hidden_dim) # A single dimension may be -1, in which case it’s inferred from the remaining dimensions and the number of elements in hs.\r\n",
        "        # hidden1 = hidden1.unsqueeze(1)\r\n",
        "        # print(hidden1.shape)\r\n",
        "        # _, (hidden2, _) = self.lstm2(hidden1)\r\n",
        "\r\n",
        "        #hidden = [1, batch size, hid dim]\r\n",
        "        #cell = [1, batch size, hid dim]\r\n",
        "        # hidden2 = hidden2.reshape(2*self.hidden_dim) # A single dimension may be -1, in which case it’s inferred from the remaining dimensions and the number of elements in hs.\r\n",
        "        #outputs are always from the top hidden layer\r\n",
        "        \r\n",
        "        # print(hidden2.shape)\r\n",
        "        #prediction = [batch size, 1]\r\n",
        "        return self.classifier(hidden1)\r\n",
        "        \r\n",
        "pred_model = PredictionModel(embed_dim, 128, vocab_size).to(device)\r\n",
        "pred_model.load_state_dict(torch.load('/content/gdrive/My Drive/NL3.14/resources/prediction_model'))\r\n",
        "pred_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionModel(\n",
              "  (embedding): Embedding(145338, 300)\n",
              "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
              "  (classifier): Linear(in_features=512, out_features=145338, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZJZGDJUOiUj"
      },
      "source": [
        "# criterion = nn.CrossEntropyLoss().to(device)\r\n",
        "optimizer = AdamW(pred_model.parameters(), lr=1e-4, eps=1e-7)\r\n",
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "# optimizer = torch.optim.Adam(pred_model.parameters(), lr=1e-4, weight_decay=1e-4)\r\n",
        "\r\n",
        "#Calculate loss on validation data\r\n",
        "def valid_loss(model, dl):\r\n",
        "  #Switch model to evaluation mode and then back to train\r\n",
        "  model.eval()\r\n",
        "  loss = 0\r\n",
        "  with torch.no_grad(): # tells Pytorch not to store values of intermediate computations for backward pass because we not gonna need gradients.\r\n",
        "    batches = math.ceil(len(dl) / batch_size)\r\n",
        "    for x, y in next_validation_generator:\r\n",
        "        preds = model(x)\r\n",
        "        loss += criterion(preds.squeeze(0), y.squeeze(1)).item()\r\n",
        "  model.train()\r\n",
        "  return loss / batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbfho_dk7sWo"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self, input_size, hidden_size, bidirectional = True):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "    self.input_size = input_size\r\n",
        "    self.bidirectional = bidirectional\r\n",
        "    \r\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, bidirectional = bidirectional)\r\n",
        "  \r\n",
        "  def forward(self, inputs, hidden):\r\n",
        "    \r\n",
        "    output, hidden = self.lstm(inputs.view(1, 1, self.input_size), hidden)\r\n",
        "    return output, hidden\r\n",
        "    \r\n",
        "  def init_hidden(self):\r\n",
        "    return (torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size),\r\n",
        "      torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size))\r\n",
        "    \r\n",
        "class AttentionDecoder(nn.Module):\r\n",
        "  \r\n",
        "  def __init__(self, hidden_size, output_size, vocab_size):\r\n",
        "    super(AttentionDecoder, self).__init__()\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "    self.output_size = output_size\r\n",
        "    \r\n",
        "    self.attn = nn.Linear(hidden_size + output_size, 1)\r\n",
        "    self.lstm = nn.LSTM(hidden_size + vocab_size, output_size) #if we are using embedding hidden_size should be added with embedding of vocab size\r\n",
        "    self.final = nn.Linear(output_size, vocab_size)\r\n",
        "  \r\n",
        "  def init_hidden(self):\r\n",
        "    return (torch.zeros(1, 1, self.output_size),\r\n",
        "      torch.zeros(1, 1, self.output_size))\r\n",
        "  \r\n",
        "  def forward(self, decoder_hidden, encoder_outputs, input):\r\n",
        "    \r\n",
        "    weights = []\r\n",
        "    for i in range(len(encoder_outputs)):\r\n",
        "      print(decoder_hidden[0][0].shape)\r\n",
        "      print(encoder_outputs[0].shape)\r\n",
        "      weights.append(self.attn(torch.cat((decoder_hidden[0][0], \r\n",
        "                                          encoder_outputs[i]), dim = 1)))\r\n",
        "    normalized_weights = F.softmax(torch.cat(weights, 1), 1)\r\n",
        "    \r\n",
        "    attn_applied = torch.bmm(normalized_weights.unsqueeze(1),\r\n",
        "                             encoder_outputs.view(1, -1, self.hidden_size))\r\n",
        "    \r\n",
        "    input_lstm = torch.cat((attn_applied[0], input[0]), dim = 1) #if we are using embedding, use embedding of input here instead\r\n",
        "    \r\n",
        "    output, hidden = self.lstm(input_lstm.unsqueeze(0), decoder_hidden)\r\n",
        "    \r\n",
        "    output = self.final(output[0])\r\n",
        "    \r\n",
        "    return output, hidden, normalized_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fy20Do2uQCb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b0e499fa-c4ce-46d8-813b-f0e1ff1f960b"
      },
      "source": [
        "from datetime import datetime\r\n",
        "# Switch model to train mode\r\n",
        "pred_model.train()\r\n",
        "\r\n",
        "validation_loss = None\r\n",
        "for epoch in range(1, 5):\r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    epoch_f_score = 0\r\n",
        "    batches = math.ceil(len(train) / batch_size)\r\n",
        "    print(batches, \"batches\")\r\n",
        "    t = datetime.now()\r\n",
        "    i = 0\r\n",
        "    for x, y in next_training_generator:\r\n",
        "        \r\n",
        "        # if we don't do this, pytorch will accumulate gradients (summation) from previous backprop to next backprop and so on...\r\n",
        "        # this behaviour is useful when we can't fit many samples in memory but we need to compute gradient on large batch. In this case, we simply accumulate gradient and do backprop\r\n",
        "        # after enough samples has been seen.\r\n",
        "        optimizer.zero_grad()\r\n",
        "        #forward\r\n",
        "        output = pred_model(x)\r\n",
        "        # print(output)\r\n",
        "        # print(output.long())\r\n",
        "        # print(y.shape)\r\n",
        "        loss = criterion(output.squeeze(0), y.squeeze(1))\r\n",
        "        # print(output.loss.item())\r\n",
        "        #calculate accuracy\r\n",
        "        # acc = calculate_accuracy(log, y)\r\n",
        "\r\n",
        "        #Back propagation\r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        #Gradient step\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        # epoch_acc += acc.item()\r\n",
        "        \r\n",
        "        if (i + 1) % 200 == 0:\r\n",
        "            dev_loss = 0 #valid_loss(pred_model, validation)\r\n",
        "            if validation_loss is None or validation_loss > dev_loss:\r\n",
        "                validation_loss = dev_loss\r\n",
        "                # Save best model\r\n",
        "                # torch.save(pred_model.state_dict(), '/content/gdrive/My Drive/NL3.14/resources/prediction_model') \r\n",
        "            print(f'Epoch {epoch} batch {i} | Avg Train Loss: {epoch_loss/(i + 1):.6f} | Current validation Loss {dev_loss:.6f}| Minimal validation Loss {validation_loss:.6f} | Current Perplexity {torch.exp(torch.tensor(dev_loss))} | Time passed {datetime.now() - t}')\r\n",
        "        i+=1\r\n",
        "    print(f'Epoch {epoch} | Avg Train Loss: {epoch_loss/batches:.6f} | Minimal validation Loss {validation_loss:.6f}')  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4157 batches\n",
            "Epoch 1 batch 199 | Avg Train Loss: 0.333203 | Current validation Loss 0.196487| Minimal validation Loss 0.196487 | Current Perplexity 1.2171193361282349 | Time passed 0:01:26.556693\n",
            "Epoch 1 batch 399 | Avg Train Loss: 0.172870 | Current validation Loss 0.053391| Minimal validation Loss 0.053391 | Current Perplexity 1.0548418760299683 | Time passed 0:02:53.646495\n",
            "Epoch 1 batch 599 | Avg Train Loss: 0.116824 | Current validation Loss 0.026861| Minimal validation Loss 0.026861 | Current Perplexity 1.0272254943847656 | Time passed 0:04:21.067256\n",
            "Epoch 1 batch 799 | Avg Train Loss: 0.088285 | Current validation Loss 0.016972| Minimal validation Loss 0.016972 | Current Perplexity 1.0171165466308594 | Time passed 0:05:48.070087\n",
            "Epoch 1 batch 999 | Avg Train Loss: 0.070985 | Current validation Loss 0.011909| Minimal validation Loss 0.011909 | Current Perplexity 1.011980414390564 | Time passed 0:07:15.232507\n",
            "Epoch 1 batch 1199 | Avg Train Loss: 0.059370 | Current validation Loss 0.008968| Minimal validation Loss 0.008968 | Current Perplexity 1.0090084075927734 | Time passed 0:08:42.102359\n",
            "Epoch 1 batch 1399 | Avg Train Loss: 0.051031 | Current validation Loss 0.007031| Minimal validation Loss 0.007031 | Current Perplexity 1.007055640220642 | Time passed 0:10:08.838812\n",
            "Epoch 1 batch 1599 | Avg Train Loss: 0.044751 | Current validation Loss 0.005775| Minimal validation Loss 0.005775 | Current Perplexity 1.0057915449142456 | Time passed 0:11:35.764782\n",
            "Epoch 1 batch 1799 | Avg Train Loss: 0.039853 | Current validation Loss 0.004880| Minimal validation Loss 0.004880 | Current Perplexity 1.0048919916152954 | Time passed 0:13:03.098872\n",
            "Epoch 1 batch 1999 | Avg Train Loss: 0.035924 | Current validation Loss 0.004210| Minimal validation Loss 0.004210 | Current Perplexity 1.0042191743850708 | Time passed 0:14:30.128284\n",
            "Epoch 1 batch 2199 | Avg Train Loss: 0.032703 | Current validation Loss 0.003693| Minimal validation Loss 0.003693 | Current Perplexity 1.0036994218826294 | Time passed 0:15:57.579353\n",
            "Epoch 1 batch 2399 | Avg Train Loss: 0.030015 | Current validation Loss 0.003283| Minimal validation Loss 0.003283 | Current Perplexity 1.0032883882522583 | Time passed 0:17:24.906746\n",
            "Epoch 1 batch 2599 | Avg Train Loss: 0.027736 | Current validation Loss 0.002952| Minimal validation Loss 0.002952 | Current Perplexity 1.002956509590149 | Time passed 0:18:52.011639\n",
            "Epoch 1 batch 2799 | Avg Train Loss: 0.025780 | Current validation Loss 0.002680| Minimal validation Loss 0.002680 | Current Perplexity 1.0026839971542358 | Time passed 0:20:19.589355\n",
            "Epoch 1 batch 2999 | Avg Train Loss: 0.024083 | Current validation Loss 0.002454| Minimal validation Loss 0.002454 | Current Perplexity 1.0024571418762207 | Time passed 0:21:46.718228\n",
            "Epoch 1 batch 3199 | Avg Train Loss: 0.022596 | Current validation Loss 0.002262| Minimal validation Loss 0.002262 | Current Perplexity 1.002264380455017 | Time passed 0:23:13.988558\n",
            "Epoch 1 batch 3399 | Avg Train Loss: 0.021283 | Current validation Loss 0.002098| Minimal validation Loss 0.002098 | Current Perplexity 1.0021005868911743 | Time passed 0:24:40.953017\n",
            "Epoch 1 batch 3599 | Avg Train Loss: 0.020114 | Current validation Loss 0.001959| Minimal validation Loss 0.001959 | Current Perplexity 1.0019607543945312 | Time passed 0:26:08.449221\n",
            "Epoch 1 batch 3799 | Avg Train Loss: 0.019068 | Current validation Loss 0.001838| Minimal validation Loss 0.001838 | Current Perplexity 1.0018397569656372 | Time passed 0:27:34.980505\n",
            "Epoch 1 batch 3999 | Avg Train Loss: 0.018126 | Current validation Loss 0.001733| Minimal validation Loss 0.001733 | Current Perplexity 1.001734733581543 | Time passed 0:29:02.711263\n",
            "Epoch 1 batch 4199 | Avg Train Loss: 0.017273 | Current validation Loss 0.001642| Minimal validation Loss 0.001642 | Current Perplexity 1.001643419265747 | Time passed 0:30:29.621918\n",
            "Epoch 1 batch 4399 | Avg Train Loss: 0.016497 | Current validation Loss 0.001562| Minimal validation Loss 0.001562 | Current Perplexity 1.0015634298324585 | Time passed 0:31:56.431514\n",
            "Epoch 1 batch 4599 | Avg Train Loss: 0.015788 | Current validation Loss 0.001492| Minimal validation Loss 0.001492 | Current Perplexity 1.0014926195144653 | Time passed 0:33:23.179099\n",
            "Epoch 1 batch 4799 | Avg Train Loss: 0.015138 | Current validation Loss 0.001429| Minimal validation Loss 0.001429 | Current Perplexity 1.001429796218872 | Time passed 0:34:50.169382\n",
            "Epoch 1 batch 4999 | Avg Train Loss: 0.014539 | Current validation Loss 0.001373| Minimal validation Loss 0.001373 | Current Perplexity 1.001373529434204 | Time passed 0:36:17.195571\n",
            "Epoch 1 batch 5199 | Avg Train Loss: 0.013987 | Current validation Loss 0.001322| Minimal validation Loss 0.001322 | Current Perplexity 1.0013232231140137 | Time passed 0:37:43.793610\n",
            "Epoch 1 batch 5399 | Avg Train Loss: 0.013475 | Current validation Loss 0.001277| Minimal validation Loss 0.001277 | Current Perplexity 1.0012776851654053 | Time passed 0:39:11.067874\n",
            "Epoch 1 batch 5599 | Avg Train Loss: 0.012999 | Current validation Loss 0.001236| Minimal validation Loss 0.001236 | Current Perplexity 1.0012366771697998 | Time passed 0:40:37.747651\n",
            "Epoch 1 batch 5799 | Avg Train Loss: 0.012556 | Current validation Loss 0.001199| Minimal validation Loss 0.001199 | Current Perplexity 1.0011993646621704 | Time passed 0:42:04.753624\n",
            "Epoch 1 batch 5999 | Avg Train Loss: 0.012142 | Current validation Loss 0.001165| Minimal validation Loss 0.001165 | Current Perplexity 1.0011653900146484 | Time passed 0:43:31.649319\n",
            "Epoch 1 batch 6199 | Avg Train Loss: 0.011755 | Current validation Loss 0.001134| Minimal validation Loss 0.001134 | Current Perplexity 1.0011345148086548 | Time passed 0:44:58.708044\n",
            "Epoch 1 batch 6399 | Avg Train Loss: 0.011392 | Current validation Loss 0.001105| Minimal validation Loss 0.001105 | Current Perplexity 1.0011060237884521 | Time passed 0:46:25.554337\n",
            "Epoch 1 batch 6599 | Avg Train Loss: 0.011051 | Current validation Loss 0.001079| Minimal validation Loss 0.001079 | Current Perplexity 1.0010799169540405 | Time passed 0:47:52.178613\n",
            "Epoch 1 batch 6799 | Avg Train Loss: 0.010730 | Current validation Loss 0.001055| Minimal validation Loss 0.001055 | Current Perplexity 1.0010559558868408 | Time passed 0:49:18.868233\n",
            "Epoch 1 batch 6999 | Avg Train Loss: 0.010427 | Current validation Loss 0.001033| Minimal validation Loss 0.001033 | Current Perplexity 1.0010336637496948 | Time passed 0:50:45.052018\n",
            "Epoch 1 batch 7199 | Avg Train Loss: 0.010141 | Current validation Loss 0.001013| Minimal validation Loss 0.001013 | Current Perplexity 1.0010130405426025 | Time passed 0:52:11.916813\n",
            "Epoch 1 batch 7399 | Avg Train Loss: 0.009870 | Current validation Loss 0.000994| Minimal validation Loss 0.000994 | Current Perplexity 1.000994086265564 | Time passed 0:53:39.589050\n",
            "Epoch 1 batch 7599 | Avg Train Loss: 0.009614 | Current validation Loss 0.000976| Minimal validation Loss 0.000976 | Current Perplexity 1.0009762048721313 | Time passed 0:55:06.663684\n",
            "Epoch 1 batch 7799 | Avg Train Loss: 0.009371 | Current validation Loss 0.000959| Minimal validation Loss 0.000959 | Current Perplexity 1.0009596347808838 | Time passed 0:56:33.518868\n",
            "Epoch 1 batch 7999 | Avg Train Loss: 0.009139 | Current validation Loss 0.000943| Minimal validation Loss 0.000943 | Current Perplexity 1.000943899154663 | Time passed 0:58:00.538335\n",
            "Epoch 1 batch 8199 | Avg Train Loss: 0.008919 | Current validation Loss 0.000928| Minimal validation Loss 0.000928 | Current Perplexity 1.000928521156311 | Time passed 0:59:27.317710\n",
            "Epoch 1 batch 8399 | Avg Train Loss: 0.008710 | Current validation Loss 0.000913| Minimal validation Loss 0.000913 | Current Perplexity 1.000913143157959 | Time passed 1:00:54.027990\n",
            "Epoch 1 batch 8599 | Avg Train Loss: 0.008510 | Current validation Loss 0.000899| Minimal validation Loss 0.000899 | Current Perplexity 1.0008996725082397 | Time passed 1:02:20.870377\n",
            "Epoch 1 batch 8799 | Avg Train Loss: 0.008319 | Current validation Loss 0.000888| Minimal validation Loss 0.000888 | Current Perplexity 1.0008878707885742 | Time passed 1:03:48.284901\n",
            "Epoch 1 batch 8999 | Avg Train Loss: 0.008136 | Current validation Loss 0.000877| Minimal validation Loss 0.000877 | Current Perplexity 1.000877022743225 | Time passed 1:05:15.643867\n",
            "Epoch 1 batch 9199 | Avg Train Loss: 0.007962 | Current validation Loss 0.000866| Minimal validation Loss 0.000866 | Current Perplexity 1.0008667707443237 | Time passed 1:06:42.441518\n",
            "Epoch 1 batch 9399 | Avg Train Loss: 0.007795 | Current validation Loss 0.000857| Minimal validation Loss 0.000857 | Current Perplexity 1.0008571147918701 | Time passed 1:08:09.094082\n",
            "Epoch 1 batch 9599 | Avg Train Loss: 0.007635 | Current validation Loss 0.000847| Minimal validation Loss 0.000847 | Current Perplexity 1.0008474588394165 | Time passed 1:09:36.086562\n",
            "Epoch 1 batch 9799 | Avg Train Loss: 0.007481 | Current validation Loss 0.000837| Minimal validation Loss 0.000837 | Current Perplexity 1.0008374452590942 | Time passed 1:11:03.046637\n",
            "Epoch 1 batch 9999 | Avg Train Loss: 0.007333 | Current validation Loss 0.000829| Minimal validation Loss 0.000829 | Current Perplexity 1.0008288621902466 | Time passed 1:12:30.202424\n",
            "Epoch 1 batch 10199 | Avg Train Loss: 0.007192 | Current validation Loss 0.000821| Minimal validation Loss 0.000821 | Current Perplexity 1.0008214712142944 | Time passed 1:13:57.900570\n",
            "Epoch 1 batch 10399 | Avg Train Loss: 0.007055 | Current validation Loss 0.000814| Minimal validation Loss 0.000814 | Current Perplexity 1.0008141994476318 | Time passed 1:15:25.912573\n",
            "Epoch 1 batch 10599 | Avg Train Loss: 0.006924 | Current validation Loss 0.000807| Minimal validation Loss 0.000807 | Current Perplexity 1.000807523727417 | Time passed 1:16:52.405656\n",
            "Epoch 1 batch 10799 | Avg Train Loss: 0.006798 | Current validation Loss 0.000801| Minimal validation Loss 0.000801 | Current Perplexity 1.0008012056350708 | Time passed 1:18:19.827744\n",
            "Epoch 1 batch 10999 | Avg Train Loss: 0.006676 | Current validation Loss 0.000795| Minimal validation Loss 0.000795 | Current Perplexity 1.0007952451705933 | Time passed 1:19:46.465289\n",
            "Epoch 1 batch 11199 | Avg Train Loss: 0.006559 | Current validation Loss 0.000789| Minimal validation Loss 0.000789 | Current Perplexity 1.0007894039154053 | Time passed 1:21:13.329032\n",
            "Epoch 1 batch 11399 | Avg Train Loss: 0.006445 | Current validation Loss 0.000783| Minimal validation Loss 0.000783 | Current Perplexity 1.0007834434509277 | Time passed 1:22:40.476380\n",
            "Epoch 1 batch 11599 | Avg Train Loss: 0.006336 | Current validation Loss 0.000776| Minimal validation Loss 0.000776 | Current Perplexity 1.0007760524749756 | Time passed 1:24:06.888818\n",
            "Epoch 1 batch 11799 | Avg Train Loss: 0.006230 | Current validation Loss 0.000769| Minimal validation Loss 0.000769 | Current Perplexity 1.0007697343826294 | Time passed 1:25:32.888041\n",
            "Epoch 1 batch 11999 | Avg Train Loss: 0.006128 | Current validation Loss 0.000765| Minimal validation Loss 0.000765 | Current Perplexity 1.0007648468017578 | Time passed 1:26:58.842551\n",
            "Epoch 1 batch 12199 | Avg Train Loss: 0.006029 | Current validation Loss 0.000760| Minimal validation Loss 0.000760 | Current Perplexity 1.0007603168487549 | Time passed 1:28:24.519267\n",
            "Epoch 1 batch 12399 | Avg Train Loss: 0.005933 | Current validation Loss 0.000756| Minimal validation Loss 0.000756 | Current Perplexity 1.000756025314331 | Time passed 1:29:50.429968\n",
            "Epoch 1 batch 12599 | Avg Train Loss: 0.005840 | Current validation Loss 0.000752| Minimal validation Loss 0.000752 | Current Perplexity 1.0007519721984863 | Time passed 1:31:16.539886\n",
            "Epoch 1 batch 12799 | Avg Train Loss: 0.005751 | Current validation Loss 0.000748| Minimal validation Loss 0.000748 | Current Perplexity 1.0007480382919312 | Time passed 1:32:43.472777\n",
            "Epoch 1 batch 12999 | Avg Train Loss: 0.005664 | Current validation Loss 0.000744| Minimal validation Loss 0.000744 | Current Perplexity 1.000744342803955 | Time passed 1:34:10.411345\n",
            "Epoch 1 batch 13199 | Avg Train Loss: 0.005579 | Current validation Loss 0.000740| Minimal validation Loss 0.000740 | Current Perplexity 1.000740647315979 | Time passed 1:35:36.428206\n",
            "Epoch 1 batch 13399 | Avg Train Loss: 0.005497 | Current validation Loss 0.000737| Minimal validation Loss 0.000737 | Current Perplexity 1.0007373094558716 | Time passed 1:37:03.266653\n",
            "Epoch 1 batch 13599 | Avg Train Loss: 0.005418 | Current validation Loss 0.000734| Minimal validation Loss 0.000734 | Current Perplexity 1.0007339715957642 | Time passed 1:38:29.754830\n",
            "Epoch 1 batch 13799 | Avg Train Loss: 0.005341 | Current validation Loss 0.000731| Minimal validation Loss 0.000731 | Current Perplexity 1.0007308721542358 | Time passed 1:39:56.519300\n",
            "Epoch 1 batch 13999 | Avg Train Loss: 0.005266 | Current validation Loss 0.000728| Minimal validation Loss 0.000728 | Current Perplexity 1.0007277727127075 | Time passed 1:41:23.243084\n",
            "Epoch 1 batch 14199 | Avg Train Loss: 0.005193 | Current validation Loss 0.000725| Minimal validation Loss 0.000725 | Current Perplexity 1.0007249116897583 | Time passed 1:42:49.990720\n",
            "Epoch 1 batch 14399 | Avg Train Loss: 0.005122 | Current validation Loss 0.000722| Minimal validation Loss 0.000722 | Current Perplexity 1.000722050666809 | Time passed 1:44:17.422095\n",
            "Epoch 1 batch 14599 | Avg Train Loss: 0.005053 | Current validation Loss 0.000719| Minimal validation Loss 0.000719 | Current Perplexity 1.0007193088531494 | Time passed 1:45:44.463184\n",
            "Epoch 1 batch 14799 | Avg Train Loss: 0.004986 | Current validation Loss 0.000716| Minimal validation Loss 0.000716 | Current Perplexity 1.0007166862487793 | Time passed 1:47:10.937023\n",
            "Epoch 1 batch 14999 | Avg Train Loss: 0.004921 | Current validation Loss 0.000714| Minimal validation Loss 0.000714 | Current Perplexity 1.0007141828536987 | Time passed 1:48:37.753205\n",
            "Epoch 1 batch 15199 | Avg Train Loss: 0.004857 | Current validation Loss 0.000711| Minimal validation Loss 0.000711 | Current Perplexity 1.0007116794586182 | Time passed 1:50:04.277942\n",
            "Epoch 1 batch 15399 | Avg Train Loss: 0.004795 | Current validation Loss 0.000709| Minimal validation Loss 0.000709 | Current Perplexity 1.0007092952728271 | Time passed 1:51:31.110483\n",
            "Epoch 1 batch 15599 | Avg Train Loss: 0.004735 | Current validation Loss 0.000707| Minimal validation Loss 0.000707 | Current Perplexity 1.0007070302963257 | Time passed 1:52:58.233260\n",
            "Epoch 1 batch 15799 | Avg Train Loss: 0.004676 | Current validation Loss 0.000704| Minimal validation Loss 0.000704 | Current Perplexity 1.0007047653198242 | Time passed 1:54:24.298931\n",
            "Epoch 1 batch 15999 | Avg Train Loss: 0.004619 | Current validation Loss 0.000702| Minimal validation Loss 0.000702 | Current Perplexity 1.0007025003433228 | Time passed 1:55:50.851092\n",
            "Epoch 1 batch 16199 | Avg Train Loss: 0.004563 | Current validation Loss 0.000700| Minimal validation Loss 0.000700 | Current Perplexity 1.0007004737854004 | Time passed 1:57:17.892213\n",
            "Epoch 1 batch 16399 | Avg Train Loss: 0.004508 | Current validation Loss 0.000698| Minimal validation Loss 0.000698 | Current Perplexity 1.000698447227478 | Time passed 1:58:44.657615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-3b5f05dea993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mdev_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mdev_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7f6a11738aee>\u001b[0m in \u001b[0;36mvalid_loss\u001b[0;34m(model, dl)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_validation_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-d6ace8065b27>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#embedded = [batch size, src len, emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# print(hidden1.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mhidden1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# A single dimension may be -1, in which case it’s inferred from the remaining dimensions and the number of elements in hs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 582\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}